{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3dc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641273e",
   "metadata": {},
   "source": [
    "## CustomLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1ddee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatParrotLink(BaseChatModel):\n",
    "    \n",
    "    model_name: str = Field(alias=\"model\")\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "    parrot_buffer_length: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "    temperature: Optional[float] = None\n",
    "    max_tokens: Optional[int] = None\n",
    "    timeout: Optional[int] = None\n",
    "    stop: Optional[List[str]] = None\n",
    "    max_retries: int = 2\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        last_message = messages[-1]\n",
    "        print(type(last_message))\n",
    "        tokens = last_message.content[: self.parrot_buffer_length]\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "        ct_output_tokens = len(tokens)\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload to the message\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "                \"model_name\": self.model_name,\n",
    "            },\n",
    "            usage_metadata={\n",
    "                \"input_tokens\": ct_input_tokens,\n",
    "                \"output_tokens\": ct_output_tokens,\n",
    "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n",
    "            },\n",
    "        )\n",
    "        ##\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        last_message = messages[-1]\n",
    "        tokens = str(last_message.content[: self.parrot_buffer_length])\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "\n",
    "        for token in tokens:\n",
    "            usage_metadata = UsageMetadata(\n",
    "                {\n",
    "                    \"input_tokens\": ct_input_tokens,\n",
    "                    \"output_tokens\": 1,\n",
    "                    \"total_tokens\": ct_input_tokens + 1,\n",
    "                }\n",
    "            )\n",
    "            ct_input_tokens = 0\n",
    "            chunk = ChatGenerationChunk(\n",
    "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n",
    "            )\n",
    "\n",
    "            if run_manager:\n",
    "                # This is optional in newer versions of LangChain\n",
    "                # The on_llm_new_token will be called automatically\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "        # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(\n",
    "                content=\"\",\n",
    "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n",
    "            )\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20b348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-eff55b08-f020-4679-bb7c-5c5ef230c6ad-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hello!\"),\n",
    "        AIMessage(content=\"Hi there human!\"),\n",
    "        HumanMessage(content=\"Meow!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9e407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-c74bc94e-fd6c-4336-b7a9-9d77a8aba583-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77a6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-b9de8e80-4c0b-469c-ab8a-ffa5dd5f9f20-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\n",
       " AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-b470ea1b-20e3-4941-916e-2b30f9a19afc-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch([\"hello\", \"goodbye\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0dd03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h|o|r||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"horse\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c44c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c|a|t||"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream(\"cat\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ff736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-9388bf2d-f17b-4512-856c-5bb62878b44d', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-9388bf2d-f17b-4512-856c-5bb62878b44d', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-9388bf2d-f17b-4512-856c-5bb62878b44d', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3, 'model_name': 'my_custom_model'}, id='run-9388bf2d-f17b-4512-856c-5bb62878b44d')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '9388bf2d-f17b-4512-856c-5bb62878b44d', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3, 'model_name': 'my_custom_model'}, id='run-9388bf2d-f17b-4512-856c-5bb62878b44d', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "async for event in model.astream_events(\"cat\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1214f358",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m tools = [tool]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# llm = ChatOpenAI(model=\"gpt-4o-mini\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m llm_with_tools = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\0_Projects\\LangGraph\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1315\u001b[39m, in \u001b[36mBaseChatModel.bind_tools\u001b[39m\u001b[34m(self, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_tools\u001b[39m(\n\u001b[32m   1298\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1299\u001b[39m     tools: Sequence[\n\u001b[32m   (...)\u001b[39m\u001b[32m   1304\u001b[39m     **kwargs: Any,\n\u001b[32m   1305\u001b[39m ) -> Runnable[LanguageModelInput, BaseMessage]:\n\u001b[32m   1306\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Bind tools to the model.\u001b[39;00m\n\u001b[32m   1307\u001b[39m \n\u001b[32m   1308\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1313\u001b[39m \u001b[33;03m        A Runnable that returns a message.\u001b[39;00m\n\u001b[32m   1314\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "load_dotenv()\n",
    "tool = TavilySearchResults(max_results = 3)\n",
    "tools = [tool]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4724f43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tavily_search_results_json',\n",
       " 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
       " 'args_schema': langchain_community.tools.tavily_search.tool.TavilyInput,\n",
       " 'return_direct': False,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'handle_tool_error': False,\n",
       " 'handle_validation_error': False,\n",
       " 'response_format': 'content_and_artifact',\n",
       " 'max_results': 3,\n",
       " 'search_depth': 'advanced',\n",
       " 'include_domains': [],\n",
       " 'exclude_domains': [],\n",
       " 'include_answer': False,\n",
       " 'include_raw_content': False,\n",
       " 'include_images': False,\n",
       " 'api_wrapper': {'tavily_api_key': SecretStr('**********')}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1632ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "def add_numbers2(a: Annotated[int, \"first number to add\"], b: Annotated[int, \"second number to add\"]) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d9b6f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add_numbers2',\n",
       " 'description': 'Add two numbers',\n",
       " 'args_schema': langchain_core.utils.pydantic.add_numbers2,\n",
       " 'return_direct': False,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'handle_tool_error': False,\n",
       " 'handle_validation_error': False,\n",
       " 'response_format': 'content',\n",
       " 'func': <function __main__.add_numbers2(a: typing.Annotated[int, 'first number to add'], b: typing.Annotated[int, 'second number to add']) -> int>,\n",
       " 'coroutine': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numbers2.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c45eb2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add_numbers',\n",
       " 'description': 'Add two numbers',\n",
       " 'args_schema': langchain_core.utils.pydantic.add_numbers,\n",
       " 'return_direct': False,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'handle_tool_error': False,\n",
       " 'handle_validation_error': False,\n",
       " 'response_format': 'content',\n",
       " 'func': <function __main__.add_numbers(a: int, b: int) -> int>,\n",
       " 'coroutine': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numbers.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4341f80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numbers2(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90e251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
