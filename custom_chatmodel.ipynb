{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3dc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    \n",
    ")\n",
    "from langchain_core.messages.ai import UsageMetadata\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641273e",
   "metadata": {},
   "source": [
    "## CustomLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a50233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 한글 테스트 이거슨 malgun gothic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc1ddee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Literal, Sequence, Union\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "\n",
    "class ChatParrotLink(BaseChatModel):\n",
    "    \n",
    "    model_name: str = Field(alias=\"model\")\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "    parrot_buffer_length: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "    temperature: Optional[float] = None\n",
    "    max_tokens: Optional[int] = None\n",
    "    timeout: Optional[int] = None\n",
    "    stop: Optional[List[str]] = None\n",
    "    max_retries: int = 2\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        last_message = messages[-1]\n",
    "        print(type(last_message))\n",
    "        tokens = last_message.content[: self.parrot_buffer_length]\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "        ct_output_tokens = len(tokens)\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload to the message\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "                \"model_name\": self.model_name,\n",
    "            },\n",
    "            usage_metadata={\n",
    "                \"input_tokens\": ct_input_tokens,\n",
    "                \"output_tokens\": ct_output_tokens,\n",
    "                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n",
    "            },\n",
    "        )\n",
    "        ##\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        last_message = messages[-1]\n",
    "        tokens = str(last_message.content[: self.parrot_buffer_length])\n",
    "        ct_input_tokens = sum(len(message.content) for message in messages)\n",
    "\n",
    "        for token in tokens:\n",
    "            usage_metadata = UsageMetadata(\n",
    "                {\n",
    "                    \"input_tokens\": ct_input_tokens,\n",
    "                    \"output_tokens\": 1,\n",
    "                    \"total_tokens\": ct_input_tokens + 1,\n",
    "                }\n",
    "            )\n",
    "            ct_input_tokens = 0\n",
    "            chunk = ChatGenerationChunk(\n",
    "                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n",
    "            )\n",
    "\n",
    "            if run_manager:\n",
    "                # This is optional in newer versions of LangChain\n",
    "                # The on_llm_new_token will be called automatically\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "        # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(\n",
    "                content=\"\",\n",
    "                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n",
    "            )\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "    \n",
    "    def bind_tools(\n",
    "            self, \n",
    "            tools: Sequence[Union[dict[str, Any], type, Callable, BaseTool]],\n",
    "            *,\n",
    "            tool_choice: Optional[\n",
    "                Union[dict, str, Literal[\"auto\", \"none\", \"required\", \"any\"], bool]\n",
    "            ] = None,\n",
    "            strict: Optional[bool] = None,\n",
    "            parallel_tool_calls: Optional[bool] = None,\n",
    "            **kwargs: Any,\n",
    "        ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        formatted_tools = [\n",
    "            convert_to_openai_tool(tool, strict=strict) for tool in tools\n",
    "        ]\n",
    "        print(formatted_tools)\n",
    "\n",
    "        return super().bind(tools=formatted_tools, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d20b348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-c0928052-46af-4fa4-92f9-d02a9afcef78-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatParrotLink(parrot_buffer_length=3, model=\"my_custom_model\")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hello!\"),\n",
    "        AIMessage(content=\"Hi there human!\"),\n",
    "        HumanMessage(content=\"Meow!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c9e407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-86846f81-cfd3-4ac8-acdf-664bb6f6fe4f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e77a6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'><class 'langchain_core.messages.human.HumanMessage'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-eaa3f2cb-d1a1-4794-b9fd-723e70e32d19-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),\n",
       " AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-c8bf4b9e-ac04-412d-bc6a-f500e48d44b5-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch([\"hello\", \"goodbye\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea0dd03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h|o|r||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"horse\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c44c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c|a|t||"
     ]
    }
   ],
   "source": [
    "async for chunk in model.astream(\"cat\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6ff736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-7df3045a-d611-41af-9f5e-56a8f2935535', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-7df3045a-d611-41af-9f5e-56a8f2935535', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-7df3045a-d611-41af-9f5e-56a8f2935535', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3, 'model_name': 'my_custom_model'}, id='run-7df3045a-d611-41af-9f5e-56a8f2935535')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '7df3045a-d611-41af-9f5e-56a8f2935535', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3, 'model_name': 'my_custom_model'}, id='run-7df3045a-d611-41af-9f5e-56a8f2935535', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "async for event in model.astream_events(\"cat\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1214f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "load_dotenv()\n",
    "tool = TavilySearchResults(max_results = 3)\n",
    "tools = [tool]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f4e61fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'bound': {'name': None,\n",
       "  'disable_streaming': False,\n",
       "  'model_name': 'my_custom_model',\n",
       "  'parrot_buffer_length': 3,\n",
       "  'temperature': None,\n",
       "  'max_tokens': None,\n",
       "  'timeout': None,\n",
       "  'stop': None,\n",
       "  'max_retries': 2},\n",
       " 'kwargs': {'tools': [{'type': 'function',\n",
       "    'function': {'name': 'tavily_search_results_json',\n",
       "     'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
       "     'parameters': {'properties': {'query': {'description': 'search query to look up',\n",
       "        'type': 'string'}},\n",
       "      'required': ['query'],\n",
       "      'type': 'object'}}}]},\n",
       " 'config': {},\n",
       " 'config_factories': [],\n",
       " 'custom_input_type': None,\n",
       " 'custom_output_type': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e343312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3+2', additional_kwargs={}, response_metadata={'time_in_seconds': 3, 'model_name': 'my_custom_model'}, id='run-262b0da8-9766-424d-bd1d-aab3ecd6f587-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"3+2=?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5458eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def add_function(a: Annotated[float, \"first number to add\"], b: Annotated[float, \"second number to add\"]) -> float:\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0020b19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(add_function, \"model_json_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb526fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'add_function',\n",
       "  'description': 'Adds two numbers together.',\n",
       "  'parameters': {'properties': {'a': {'description': 'first number to add',\n",
       "     'type': 'number'},\n",
       "    'b': {'description': 'second number to add', 'type': 'number'}},\n",
       "   'required': ['a', 'b'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "convert_to_openai_tool(add_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9b118c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add_function',\n",
       " 'description': 'Adds two numbers together.',\n",
       " 'args_schema': langchain_core.utils.pydantic.add_function,\n",
       " 'return_direct': False,\n",
       " 'verbose': False,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'handle_tool_error': False,\n",
       " 'handle_validation_error': False,\n",
       " 'response_format': 'content',\n",
       " 'func': <function __main__.add_function(a: typing.Annotated[float, 'first number to add'], b: typing.Annotated[float, 'second number to add']) -> float>,\n",
       " 'coroutine': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_function.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'description': 'first number to add', 'title': 'A', 'type': 'number'},\n",
       " 'b': {'description': 'second number to add', 'title': 'B', 'type': 'number'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_function.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4724f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: kiwipiepy_model is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8130423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain.agents import create_tool_calling_agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
